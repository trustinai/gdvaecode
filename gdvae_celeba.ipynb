{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GdVAE CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:52:59.557292Z",
     "start_time": "2024-06-24T06:52:59.551242Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copyright (C) 2024 Ruhr West University of Applied Sciences, Bottrop, Germany\n",
    "# AND e:fs TechHub GmbH, Gaimersheim, Germany\n",
    "#\n",
    "# This Source Code Form is subject to the terms of the Apache License 2.0\n",
    "# If a copy of the APL2 was not distributed with this\n",
    "# file, You can obtain one at https://www.apache.org/licenses/LICENSE-2.0.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Handling all necessary imports for the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:02.520110Z",
     "start_time": "2024-06-24T06:52:59.568702Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:02.582163Z",
     "start_time": "2024-06-24T06:53:02.522358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Cuda available? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is Cuda available? {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset\n",
    "Since CelebA usually exceeds the daily quota it is recommended to download the dataset directly from their Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:26.965793Z",
     "start_time": "2024-06-24T06:53:02.583442Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "selected_classes = [31]\n",
    "data_root = 'data/'\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CelebA(data_root,\n",
    "                                            target_type='attr',\n",
    "                                            download=False,\n",
    "                                            split='train',\n",
    "                                            transform=torchvision.transforms.Compose([\n",
    "                                                torchvision.transforms.CenterCrop(178),\n",
    "                                                torchvision.transforms.Resize(64),\n",
    "                                                torchvision.transforms.ToTensor()\n",
    "                                            ]))\n",
    "test_dataset = torchvision.datasets.CelebA(data_root,\n",
    "                                            target_type='attr',\n",
    "                                            download=False,\n",
    "                                            split='test',\n",
    "                                            transform=torchvision.transforms.Compose([\n",
    "                                                torchvision.transforms.CenterCrop(178),\n",
    "                                                torchvision.transforms.Resize(64),\n",
    "                                                torchvision.transforms.ToTensor()\n",
    "                                            ]))\n",
    "# Reduce Attributes of CelebA Dataset to only include the requested class\n",
    "# e.g. Class 31 = Smiling\n",
    "train_dataset.attr = train_dataset.attr[:, selected_classes]\n",
    "test_dataset.attr = test_dataset.attr[:, selected_classes]\n",
    "\n",
    "# Init Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True,\n",
    "                                          )\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         drop_last=True,\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Helper\n",
    "torch.nn.Sequential view helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:26.973547Z",
     "start_time": "2024-06-24T06:53:26.968555Z"
    }
   },
   "outputs": [],
   "source": [
    "class ViewCeleba(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Helper function to use .view() inside of torch.nn.Sequential()\n",
    "    \"\"\"\n",
    "    def __init__(self, size: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return inputs.view(self.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA CVAE-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:26.993394Z",
     "start_time": "2024-06-24T06:53:26.980375Z"
    }
   },
   "outputs": [],
   "source": [
    "class CelebAEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder q(z|x,y) to encode a given image to the latent space \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape: Tuple[int, int, int] = (3, 64, 64),\n",
    "                 latent_size: int = 64,\n",
    "                 num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        CelebA Encoder\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): Size of Input Images\n",
    "            latent_size (int): Number of latent dimensions.\n",
    "            num_classes (int): Number of classes.\n",
    "        \"\"\"\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.in_channels = channels\n",
    "        self.in_height = height\n",
    "        self.in_width = width\n",
    "\n",
    "        self.DIM = 32\n",
    "\n",
    "        # CelebA Encoder\n",
    "        self.conv_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(self.in_channels+1, self.DIM, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),  # +1 for label encoder\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(self.DIM, self.DIM, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(self.DIM, self.DIM*2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(self.DIM*2, self.DIM*2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(self.DIM*2, self.DIM*8, kernel_size=(4, 4), stride=(1, 1)),\n",
    "            torch.nn.ReLU(True),\n",
    "            ViewCeleba((-1, self.DIM*8)), # Flatten\n",
    "            torch.nn.Linear(self.DIM*8, self.latent_size * 2),\n",
    "        )\n",
    "\n",
    "        # Label Encoder\n",
    "        self.label_encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=num_classes, out_features=1 * self.in_height * self.in_width),\n",
    "            ViewCeleba((-1, 1, self.in_height, self.in_width))\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        The forward step of the encoder returns the latent space(z) generated by x and y\n",
    "        q(z|x,y)\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): set of images\n",
    "            y (torch.tensor): set of one hot encoded class labels\n",
    "\n",
    "        Returns:\n",
    "            (torch.tensor): mean\n",
    "            (torch.tensor): log variance\n",
    "        \"\"\"\n",
    "        y = self.label_encoder(y)\n",
    "\n",
    "        # Concatenate transformed label with image along channel dimension\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "\n",
    "        # Actual encoding of x and y to latent representation z\n",
    "        x = self.conv_encoder(x)\n",
    "\n",
    "        # Split into Mean and LogVar\n",
    "        mean = x[:, :self.latent_size]\n",
    "        log_variance = x[:, self.latent_size:]\n",
    "\n",
    "        # Clamp LogVar\n",
    "        log_variance.clamp_(None, 5)\n",
    "\n",
    "        return mean, log_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA CVAE-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.004456Z",
     "start_time": "2024-06-24T06:53:26.995387Z"
    }
   },
   "outputs": [],
   "source": [
    "class CelebADecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder p(x|z,y) to decode a given latent space vector back to the image space\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self,\n",
    "                 input_shape: Tuple[int, int, int] = (3, 64, 64),\n",
    "                 latent_size: int = 64,\n",
    "                 num_classes: int = 2,\n",
    "                 conditional_decoder: bool=True):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): Size of Input Images\n",
    "            latent_size (int): Number of latent dimensions.\n",
    "            num_classes (int): Number of classes.\n",
    "            conditional_decoder (bool): If the model should use a conditional decoder\n",
    "        \"\"\"\n",
    "        self.out_channels, self.out_height, self.out_width = input_shape\n",
    "\n",
    "        self.num_features = self.out_channels * self.out_height * self.out_width\n",
    "        self.latent_size = latent_size  # n_dims_latent\n",
    "\n",
    "        self.conditional_decoder = conditional_decoder\n",
    "        self.latent_size_decoder = self.latent_size\n",
    "\n",
    "        self.DIM = 32\n",
    "\n",
    "        if self.conditional_decoder:\n",
    "            self.latent_size_decoder = self.latent_size + 1\n",
    "\n",
    "        self.conv_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.latent_size_decoder, self.DIM*8),\n",
    "            ViewCeleba((-1, self.DIM*8, 1, 1)),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(self.DIM*8, self.DIM*2, kernel_size=(4, 4)),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(self.DIM*2, self.DIM*2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(self.DIM*2, self.DIM, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(self.DIM, self.DIM, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.ConvTranspose2d(self.DIM, self.out_channels, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
    "            torch.nn.ReLU(True)\n",
    "        )\n",
    "        self.label_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=num_classes, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The forward step of the decoder returns the image(x) generated by z and y\n",
    "        p(x|z,y)\n",
    "\n",
    "        Args:\n",
    "            z (torch.tensor): set of latent space vectors\n",
    "            y (torch.tensor): set of one hot encoded class labels\n",
    "\n",
    "        Returns:\n",
    "            (torch.tensor): generated Image\n",
    "        \"\"\"\n",
    "        # Conditional Decoder\n",
    "        if self.conditional_decoder:\n",
    "            y = self.label_decoder(y)\n",
    "            z = torch.cat([z, y], dim=1)\n",
    "        # Generate Image\n",
    "        x = self.conv_decoder(z)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA Prior-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.015671Z",
     "start_time": "2024-06-24T06:53:27.006475Z"
    }
   },
   "outputs": [],
   "source": [
    "class CelebAPriorEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The Prior Encoder p(z|y) learns normal distributions for each dimension of the latent representation\n",
    "    for each class based on the class labels 'y'.\n",
    "\n",
    "    The GDA utilizes the prior distribution to classify samples drawn from the encoder distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 latent_size: int = 64,\n",
    "                 num_classes: int = 2):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): number of classes\n",
    "            latent_size (int): number of latent dimensions\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_size = latent_size\n",
    "        self.DIM = 4\n",
    "\n",
    "        self.prior_encoder_mean = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.num_classes, out_features=self.DIM),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=self.DIM, out_features=self.DIM),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=self.DIM, out_features=self.DIM),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.prior_encoder_log_variance = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=1, out_features=self.DIM),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=self.DIM, out_features=self.DIM),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=self.DIM, out_features=self.DIM),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.mean_layer = torch.nn.Linear(self.DIM, self.latent_size)\n",
    "        self.log_variance_layer = torch.nn.Linear(self.DIM, self.latent_size)\n",
    "\n",
    "    def forward(self, y: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        The forward step of the prior network returns the latent distributions p(z|y) used by the GDA.\n",
    "\n",
    "\n",
    "        Args:\n",
    "            y (torch.tensor): set of one hot encoded class labels\n",
    "\n",
    "        Returns:\n",
    "            (torch.tensor): prior mean\n",
    "            (torch.tensor): prior log variance\n",
    "        \"\"\"\n",
    "\n",
    "        z_mean = self.prior_encoder_mean(y)\n",
    "        mean = self.mean_layer(z_mean)\n",
    "\n",
    "        y_lv = torch.ones([y.shape[0], 1]).to(device)\n",
    "        z_log_variance = self.prior_encoder_log_variance(y_lv)\n",
    "        log_variance = self.log_variance_layer(z_log_variance)\n",
    "        log_variance.clamp_(None, 5)\n",
    "\n",
    "        return mean, log_variance\n",
    "\n",
    "    def get_prior_distributions_parameters(self):\n",
    "        \"\"\"\n",
    "        Return the parameters of the prior distributions\n",
    "\n",
    "        Returns:\n",
    "            (torch.tensor): mean value of the normal distributions for each class\n",
    "            (torch.tensor): standard deviation of the normal distributions for each class\n",
    "        \"\"\"\n",
    "        # identity matrix with size equal to the number of classes\n",
    "        # -> used to get the prior distributions for all classes to calculate likelihood values\n",
    "        identity_matrix = torch.eye(self.num_classes, self.num_classes).to(device)\n",
    "\n",
    "        # calculate conditional probabilities for all classes p(z | identity_matrix)\n",
    "        mean_prior, log_variance_prior = self.forward(identity_matrix)\n",
    "        # std = sqrt(e^(ln(variance)))\n",
    "        std_prior = log_variance_prior.exp().sqrt() + torch.finfo(torch.float32).eps\n",
    "        return mean_prior, std_prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.033199Z",
     "start_time": "2024-06-24T06:53:27.017215Z"
    }
   },
   "outputs": [],
   "source": [
    "class CelebAGDA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian Discriminant Analysis.\n",
    "\n",
    "    This class employs the encoder as a feature extractor, which is then classified by the GDA based \n",
    "    on the distributions of the prior network. It constitutes the implementation of Algorithm 1 \n",
    "    (an EM-based classifier) from the main paper.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 prior_network: torch.nn.Module,\n",
    "                 encoder: torch.nn.Module,\n",
    "                 train_loader: torch.utils.data.DataLoader,\n",
    "                 input_shape: Tuple[int, int, int] = (3, 64, 64),\n",
    "                 latent_size: int = 64,\n",
    "                 num_classes: int = 2,\n",
    "                 iterations: int = 3,\n",
    "                 sample_amount: int = 20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prior_network: used prior network reference\n",
    "            encoder: used encoder reference\n",
    "            train_loader: used dataloader\n",
    "            input_shape: input shape of used images\n",
    "            latent_size: Number of latent dimensions.\n",
    "            num_classes: Number of classes.\n",
    "            iterations: GDA iterations T\n",
    "            sample_amount: GDA number of samples S\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_features = input_shape[0]*input_shape[1]*input_shape[2]\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_size = latent_size\n",
    "        self.iterations = iterations\n",
    "        self.sample_amount = sample_amount\n",
    "        self.prior_network = prior_network\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Compute prior for all classes based on their likelihood of occurrence\n",
    "        _, class_counts = torch.unique(train_loader.dataset.attr, return_counts=True)\n",
    "        #determine p(y)\n",
    "        class_ratios = class_counts / class_counts.sum()\n",
    "\n",
    "        # Restore dimensions used by the previously implemented code\n",
    "        class_ratios = class_ratios.unsqueeze(dim=0).T\n",
    "        # Keep dtype consistent\n",
    "        class_ratios = class_ratios.type(torch.float32)\n",
    "        #set prior probability p(y)\n",
    "        self.prior = class_ratios.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.tensor, use_softmax=True) -> Tuple[torch.tensor, torch.tensor, torch.tensor, torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        GDA forward pass.\n",
    "        \n",
    "        The forward pass implements Algorithm 1 from the paper and \n",
    "        generates a sample from q(z|x) that is then classified for \n",
    "        the loss calculation.\n",
    "\n",
    "        Args:\n",
    "            x : input batch\n",
    "        Returns:\n",
    "            torch.tensor: Class wise prediction for each sample in 'x'\n",
    "            torch.tensor: latent space generated for input batch\n",
    "            torch.tensor: One hot encoded predictions for each sample\n",
    "            torch.tensor: Encoder Mean for input batch\n",
    "            torch.tensor: Encoder log variance for input batch\n",
    "        \"\"\"\n",
    "\n",
    "        mean_prior, std_prior = self.prior_network.get_prior_distributions_parameters()\n",
    "\n",
    "        # Algorithm 1 (an EM-based classifier)\n",
    "        # q(y|x) Init\n",
    "        q_y_x_log = torch.transpose(torch.log(self.prior), 0, 1)\n",
    "        q_y_x_log = q_y_x_log.expand(x.shape[0], -1)\n",
    "\n",
    "        # Sampling Start\n",
    "        for iter_idx in range(self.iterations):\n",
    "            z_samples = []\n",
    "            for samp_idx in range(self.sample_amount):\n",
    "                # q(y|x)\n",
    "                y_pred = torch.nn.functional.gumbel_softmax(logits=q_y_x_log, tau=1, hard=True)\n",
    "\n",
    "                # q(z|x,y) for sampled y\n",
    "                enc_mean, enc_logvar = self.encoder(x=x, y=y_pred)\n",
    "\n",
    "                # z(s) => sampled z\n",
    "                z_sample = reparameterize(enc_mean, enc_logvar)\n",
    "                z_samples.append(z_sample)\n",
    "            z_pred = torch.stack(z_samples)\n",
    "            z_pred = torch.unsqueeze(z_pred, dim=2)\n",
    "\n",
    "            # calculate a softmax confidence score based on the samples likelihoods\n",
    "            # to be drawn from the respective distributions\n",
    "            # p(z(s)|y)\n",
    "            loglikelihood = log_prob_normal(mean_prior, std_prior, z_pred)\n",
    "\n",
    "            # log(p(z|y)=log prod_z p(z_i|y)-> sum_l_\n",
    "            loglikelihood_z_given_y = torch.sum(loglikelihood, dim=3)\n",
    "\n",
    "            # p(y|z(s)) <- p(z(s)|y) p(y)\n",
    "            loglikelihood_weighted_z_given_y = torch.add(loglikelihood_z_given_y, self.prior.log().view(1, -1))\n",
    "\n",
    "            # q(y|x) <- sum(p(y|z(s)))\n",
    "            q_y_x_log = torch.mean(loglikelihood_weighted_z_given_y, dim=0)\n",
    "\n",
    "        #After implementing Algorithm 1, use q(y|x) \n",
    "        #to generate samples for the loss calculation, \n",
    "        if self.training:\n",
    "            y_preds_hard = torch.nn.functional.gumbel_softmax(logits=q_y_x_log, tau=1, hard=True)\n",
    "            enc_means, enc_logvars = self.encoder(x=x, y=y_preds_hard)\n",
    "            z_preds = reparameterize(enc_means, enc_logvars)\n",
    "            z_preds = torch.unsqueeze(z_preds, dim=1)\n",
    "\n",
    "            # p(z(s)|y)\n",
    "            loglikelihood = log_prob_normal(mean_prior, std_prior, z_preds)\n",
    "\n",
    "            # log(p(z|y)=log prod_z p(z_i|y)-> sum_l_\n",
    "            loglikelihood_z_given_y = torch.sum(loglikelihood, dim=2)  # (r, k)\n",
    "\n",
    "            # p(y|z(s)) <- p(z(s)|y) p(y)\n",
    "            loglikelihood_weighted_z_given_y = torch.add(loglikelihood_z_given_y, self.prior.log().view(1, -1))  # (r,k)\n",
    "            y_preds = loglikelihood_weighted_z_given_y\n",
    "\n",
    "            z_preds = z_preds.squeeze(dim=1)\n",
    "        else:\n",
    "            # during inference only use encoded means and mean class prediction\n",
    "            y_preds = q_y_x_log\n",
    "            y_preds_hard = torch.argmax(y_preds, dim=1)\n",
    "            y_preds_hard = torch.nn.functional.one_hot(y_preds_hard, num_classes=self.num_classes).float()  # (n, k)\n",
    "            enc_means, enc_logvars = self.encoder(x=x, y=y_preds_hard)\n",
    "            z_preds = enc_means\n",
    "\n",
    "        if not use_softmax:\n",
    "            y_preds = q_y_x_log\n",
    "\n",
    "        return y_preds, z_preds, y_preds_hard, enc_means, enc_logvars\n",
    "\n",
    "\n",
    "def log_prob_normal(means: torch.tensor, std_prior: torch.tensor, z: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Calculates the log likelihood using a normal distribution \n",
    "        Args:\n",
    "            means_prior (torch.tensor): Mean prior value of the prior_network for both classes\n",
    "            std_prior (torch.tensor): std prior value of the prior_network for both classes\n",
    "            z (torch.tensor): latentspace value\n",
    "\n",
    "        Returns:\n",
    "            log probability\n",
    "        \"\"\"\n",
    "        log_prob_z = -0.5 * torch.log(2. * torch.tensor(torch.pi)) - torch.log(std_prior) - 0.5 * (z - means) ** 2 / (\n",
    "                    std_prior ** 2)\n",
    "        return log_prob_z\n",
    "\n",
    "\n",
    "def reparameterize(mean: torch.tensor, log_variance: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Uses mean and standard deviation to sample from a normal distribution.\n",
    "\n",
    "        Reparameterization trick: sample a value from the standard normal distribution.\n",
    "        Multiply this value with the standard deviation described by 'log_variance'\n",
    "        and add the mean.\n",
    "\n",
    "        Args:\n",
    "            mean: mean values of each latent dimension of each sample in a batch\n",
    "            log_variance: logarithm of the variance of each latent dimension of each\n",
    "                sample in a batch\n",
    "\n",
    "        Returns:\n",
    "            Set of samples derived from the described normal distributions.\n",
    "            Same size as mean and log_variance.\n",
    "\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_variance)\n",
    "        random_values = torch.randn_like(std)\n",
    "        return mean + random_values * (std + torch.finfo(torch.float32).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Counterfactual Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.049989Z",
     "start_time": "2024-06-24T06:53:27.034607Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CounterfactualGenerator:\n",
    "    \"\"\"\n",
    "    Class for generating counterfactual. Is used to save the most important features\n",
    "    \"\"\"\n",
    "    def __init__(self, prior, probability_cf:torch.tensor = torch.tensor([0.99])):\n",
    "        self.probability_cf = probability_cf\n",
    "        self.prior = prior\n",
    "        self.epsilon = torch.log(self.probability_cf / (1 - self.probability_cf))\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.mean_prior = None\n",
    "        self.std_prior = None\n",
    "    \n",
    "    \n",
    "    def local_l2(self,\n",
    "                 z_preds: torch.tensor,\n",
    "                 y_preds: torch.tensor,\n",
    "                 delta: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Determine counterfactual examples in latent space using \"walk in direction of the gradient (local_l2)\" (mode=False)\n",
    "        Args:\n",
    "            z_preds (torch.tensor): latent representations of a batch with input data\n",
    "            y_preds (torch.tensor): class prediction only used to select the classes\n",
    "            delta (torch.tensor): probability values for the generation of counterfactuals\n",
    "        Returns:\n",
    "            z_deltas (torch.tensor): latent variable for counterfactual\n",
    "        \"\"\"\n",
    "        # select the class c (predicted) and k (counterfactual)\n",
    "        c,k = get_predicted_class_and_cf_class(y_preds)\n",
    "\n",
    "        # decision function for class c -> log(p(y|c)) and class k\n",
    "        # covariances are shared\n",
    "        # f(z) = w*z + b\n",
    "        # with w = COV^-1*(mu_c-mu_k)\n",
    "        # and b=b_c-b_k with b_i =-0.5*mu_i^T *COV^-1*mu_i+log(p(y=i))\n",
    "\n",
    "        wn = self.w[c] - self.w[k]\n",
    "        b = self.b[c] - self.b[k]\n",
    "\n",
    "        # counterfactuals when walking in the direction of the gradient\n",
    "        kappa = -(torch.sum(wn * z_preds, dim=1, keepdim=True) + b - delta) / (\n",
    "                    torch.sum(wn ** 2, dim=1, keepdim=True))\n",
    "        \n",
    "        z_deltas = z_preds + kappa * wn\n",
    "\n",
    "        return z_deltas\n",
    "    \n",
    "    def glob(self,\n",
    "                 z_preds: torch.tensor,\n",
    "                 y_preds: torch.tensor,\n",
    "                 delta: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Determine counterfactual examples in latent space using the global method\n",
    "        Args:\n",
    "            z_preds (torch.tensor): latent representations of a batch with input data\n",
    "            y_preds (torch.tensor): class prediction only used to select the classes\n",
    "            delta (torch.tensor): probability values for the generation of counterfactuals\n",
    "        Returns:\n",
    "            z_deltas (torch.tensor): latent variable for counterfactual\n",
    "        \"\"\"\n",
    "        # select the class c (predicted) and k (counterfactual)\n",
    "        c,k = get_predicted_class_and_cf_class(y_preds)\n",
    "\n",
    "        # decision function for class c -> log(p(y|c)) and class k\n",
    "        # covariances are shared\n",
    "        # f(z) = w*z + b\n",
    "        # with w = COV^-1*(mu_c-mu_k)\n",
    "        # and b=b_c-b_k with b_i =-0.5*mu_i^T *COV^-1*mu_i+log(p(y=i))\n",
    "\n",
    "        wn = self.w[c] - self.w[k]\n",
    "        b = self.b[c] - self.b[k]\n",
    "        zk = self.mean_prior[k]\n",
    "        ## counterfactuals when walking to the cf baseline\n",
    "        kappa = (torch.sum(wn * z_preds, dim=1, keepdim=True) + b - delta) / (\n",
    "                torch.sum(wn * (z_preds - zk), dim=1, keepdim=True))\n",
    "        z_deltas = z_preds + kappa * (zk - z_preds)\n",
    "\n",
    "        return z_deltas\n",
    "    \n",
    "    def local_m(self,\n",
    "                z_preds: torch.Tensor,\n",
    "                y_preds: torch.Tensor,\n",
    "                delta: torch.Tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Determine counterfactual examples in latent space using the generalized version (local_m)\n",
    "        local_m optimizes the mahalanobis distance\n",
    "        Args:\n",
    "            z_preds (torch.tensor): latent representations of a batch with input data\n",
    "            y_preds (torch.tensor): class prediction only used to select the classes\n",
    "            delta (torch.tensor): probability values for the generation of counterfactuals\n",
    "        Returns:\n",
    "            z_deltas (torch.tensor): latent variable for counterfactual\n",
    "        \"\"\"\n",
    "        # select the class c (predicted) and k (counterfactual)\n",
    "        c,k = get_predicted_class_and_cf_class(y_preds)\n",
    "\n",
    "        wn = self.w[c] - self.w[k]\n",
    "        b = self.b[c] - self.b[k]\n",
    "\n",
    "        std_prior = self.std_prior[0].unsqueeze(dim=0)\n",
    "        sigma = (std_prior ** 2)\n",
    "\n",
    "        lmbd = -2*(1/torch.sum(sigma*wn*wn, dim=1, keepdim=True))*((torch.sum(-wn * (self.mean_prior[k] + z_preds - self.mean_prior[c]), dim=1, keepdim=True) - b + delta))\n",
    "\n",
    "        z_deltas = -(lmbd/2)*sigma*wn + (self.mean_prior[k] + z_preds - self.mean_prior[c])\n",
    "\n",
    "        return z_deltas\n",
    "    \n",
    "    def calculate_w_and_b(self):\n",
    "        \"\"\"\n",
    "        Calculates w and b. This is done to save time during training, since the decision boundary only changes during gradient updates.\n",
    "        After the model is completly trained w and b stay the same during interference, therefore there is no need to recalculate them with the prior_encoder\n",
    "        \"\"\"\n",
    "        if self.mean_prior is None:\n",
    "            print(\"Error Mean-Prior not set in Counterfactual Generator\")\n",
    "            raise ValueError\n",
    "        self.w = (1 / (self.std_prior ** 2)) * self.mean_prior\n",
    "        self.b = -0.5 * torch.sum(\n",
    "            self.mean_prior * (1 / (self.std_prior ** 2)) * self.mean_prior,\n",
    "            dim=1, keepdim=True) + torch.log(self.prior)\n",
    "        \n",
    "    \n",
    "    def calculate_counterfactual_delta_distribution(self,\n",
    "                                                    z_preds: torch.tensor,\n",
    "                                                    y_preds: torch.tensor) -> torch.distributions.uniform:\n",
    "        \"\"\"\n",
    "        During training we train by sampling random probabilties for the counterfactual method. \n",
    "        Args:\n",
    "            z_preds (torch.tensor): latent representations of a batch with input data\n",
    "            y_preds (torch.tensor): class prediction only used to select the classes\n",
    "        Returns:\n",
    "            delta (torch.distributions.uniform): Uniform distribution to sample from during training\n",
    "        \"\"\"\n",
    "        c,k = get_predicted_class_and_cf_class(y_preds)\n",
    "        wn = self.w[c] - self.w[k]\n",
    "        b = self.b[c] - self.b[k]\n",
    "        zk = self.mean_prior[k]\n",
    "        \n",
    "        fmin = torch.sum(wn * zk, dim=1, keepdim=True) + b\n",
    "\n",
    "        fmax = torch.sum(wn * z_preds, dim=1, keepdim=True) + b\n",
    "        epsilon_max = torch.min(fmax, self.epsilon)\n",
    "        epsilon_min = torch.max(fmin, -self.epsilon)\n",
    "        el = torch.min(epsilon_max, epsilon_min)\n",
    "        eu = torch.max(epsilon_max, epsilon_min)\n",
    "\n",
    "        el = torch.where(el < eu, el, -self.epsilon)\n",
    "        eu = torch.where(eu > el, eu, self.epsilon)\n",
    "        delta = torch.distributions.Uniform(el, eu)\n",
    "\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Counterfactual Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.058981Z",
     "start_time": "2024-06-24T06:53:27.052836Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predicted_class_and_cf_class(y_preds: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_preds (torch.tensor): class prediction\n",
    "    Returns:\n",
    "        c (torch.tensor): Original Class\n",
    "        k (torch.tensor): Counterfactual Class\n",
    "    \"\"\"\n",
    "    # Works with multiclass\n",
    "    # Make sure to get the class with the highest prediction\n",
    "    y_samples_class_idx = torch.argmax(y_preds, dim=1)\n",
    "    # Make sure to get the class with the lowest prediction\n",
    "    y_samples_neg_class_idx = torch.argmin(y_preds, dim=1)\n",
    "\n",
    "    c = y_samples_class_idx.detach()\n",
    "    k = y_samples_neg_class_idx.detach()\n",
    "    c = c.to(torch.long)\n",
    "    k = k.to(torch.long)\n",
    "    return c, k\n",
    "\n",
    "def get_prediction_delta(y_preds: torch.tensor, delta: torch.tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_preds (torch.tensor): class prediction only used to select the classes works for multiple classes\n",
    "        delta (torch.tensor): pregenerated by calculate_counterfactuals_delta\n",
    "    Returns:\n",
    "        y_preds_delta (torch.tensor): probability values formated to be passed to the decoder\n",
    "    \"\"\"\n",
    "    c, k = get_predicted_class_and_cf_class(y_preds)\n",
    "    # Create a tensor of zeros with the same shape as y_preds_delta\n",
    "    y_preds_delta = torch.zeros_like(y_preds).float()\n",
    "\n",
    "    # Set the values using advanced indexing\n",
    "    y_preds_delta[torch.arange(y_preds.shape[0]), c] = torch.sigmoid(delta.squeeze())\n",
    "    y_preds_delta[torch.arange(y_preds.shape[0]), k] = 1.0 - y_preds_delta[torch.arange(y_preds.shape[0]), c]\n",
    "\n",
    "    return y_preds_delta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GdVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.069694Z",
     "start_time": "2024-06-24T06:53:27.060778Z"
    }
   },
   "outputs": [],
   "source": [
    "class GDVAE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian Discriminant Variational Autoencoder (GDVAE).\n",
    "    \n",
    "    This class encapsulates all models essential for constructing a GDVAE, managing their interaction. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_shape: Tuple[int, int, int] = (3, 64, 64),\n",
    "                 latent_size: int = 64,\n",
    "                 num_classes: int = 2,\n",
    "                 ALPHA: float = 1.0,\n",
    "                 BETA: float = 1.0,\n",
    "                 GAMMA: float = 1.0\n",
    "                 ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            input_shape (int): input size of the images\n",
    "            latent_size (int): size of the latent space\n",
    "            num_classes (int): amount of classes\n",
    "            ALPHA (float): Weight for MSE and KLD-Loss\n",
    "            BETA (float): Weight for MSE and KLD_Prior-Loss\n",
    "            GAMMA (float): Weight for Consistency-Loss\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Init Variables\n",
    "        self.num_classes = num_classes\n",
    "        self.ALPHA = ALPHA\n",
    "        self.BETA = BETA\n",
    "        self.GAMMA = GAMMA\n",
    "\n",
    "        # Settings\n",
    "        num_pixel = (input_shape[0]*input_shape[1]*input_shape[2])\n",
    "        self.global_mode = True\n",
    "        self.sample_amount_cf = 10\n",
    "        # 10% of num_pixel\n",
    "        self.ce_weight = 0.1\n",
    "        # Weights\n",
    "        self.kld_weight = latent_size / num_pixel\n",
    "\n",
    "        # Init Probability for Counterfactuals\n",
    "        # probability range for generation of counterfactuals during training\n",
    "        # e.g. for 0.99 counterfactuals are generated in the range [0.99,0.01]\n",
    "        self.probability_cf = torch.tensor(0.99)\n",
    "    \n",
    "        # Init Dataloaders\n",
    "        self.train_loader, self.test_loader = train_loader, test_loader\n",
    "        # Init Encoder\n",
    "        self.encoder = CelebAEncoder(input_shape=input_shape, latent_size=latent_size, num_classes=num_classes)\n",
    "        # Init Decoder\n",
    "        self.decoder = CelebADecoder(input_shape=input_shape, latent_size=latent_size, num_classes=num_classes)\n",
    "        # Init Prior Network\n",
    "        self.prior_network = CelebAPriorEncoder(latent_size=latent_size, num_classes=num_classes)\n",
    "        # Init GDA Sampling Method\n",
    "        self.gda = CelebAGDA(self.prior_network, self.encoder, self.train_loader)\n",
    "        \n",
    "        self.cf_generator = CounterfactualGenerator(self.gda.prior,self.probability_cf)\n",
    "\n",
    "    def encode(self,\n",
    "               x: torch.tensor,\n",
    "               y: torch.tensor) -> \\\n",
    "            Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        Transform a batch with input images and their labels into latent representations consisting of normal\n",
    "        distribution parameters per latent dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): batch with input data\n",
    "            y (torch.tensor): one hot encoded class labels for x\n",
    "\n",
    "        Returns:\n",
    "            (torch.tensor): mean values of the latent representation\n",
    "            (torch.tensor): log(variances) of the latent representation\n",
    "        \"\"\"\n",
    "\n",
    "        mean, log_variance = self.encoder(x, y)\n",
    "        return mean, log_variance\n",
    "\n",
    "    def decode(self, z: torch.tensor, y: torch.Tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Transform a batch with scalar latent representations into reconstructions of the input images.\n",
    "\n",
    "        Args:\n",
    "            z (torch.tensor): latent representations of a batch with input data\n",
    "            y (torch.tensor): class prediction\n",
    "        Returns:\n",
    "            (torch.tensor): Reconstructions of 'x'\n",
    "        \"\"\"\n",
    "\n",
    "        reconstruction = self.decoder(z, y)\n",
    "        return reconstruction\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.074941Z",
     "start_time": "2024-06-24T06:53:27.071019Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    \"\"\"\n",
    "    Normalizes a given input image\n",
    "    Args:\n",
    "        image (torch.tensor): input_image\n",
    "    Returns:\n",
    "         torch.tensor: normalized input image\n",
    "    \"\"\"\n",
    "    image[:, :, :] -= torch.min(image[:, :, :])\n",
    "    image[:, :, :] /= torch.max(image[:, :, :])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "Includes the CVAE-Loss and the Consistency-Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.084492Z",
     "start_time": "2024-06-24T06:53:27.076373Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function_cvae(x: torch.tensor,\n",
    "                       reconstruction: torch.tensor,\n",
    "                       mean: torch.tensor,\n",
    "                       log_variance: torch.tensor,\n",
    "                       mean_prior: torch.tensor,\n",
    "                       log_variance_prior: torch.tensor,\n",
    "                       eps: float = 1.0e-5) \\\n",
    "        -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Calculates the GDVAE loss, an extended CVAE loss as described in Eq. 7+8. \n",
    "\n",
    "    Args:\n",
    "        x: Input batch.\n",
    "        reconstruction: Reconstruction (decoder output) of the input batch 'x'.\n",
    "        mean: Mean values of the latent space for each sample in 'x'.\n",
    "        log_variance: ln of the variances of the latent space for each sample in 'x'\n",
    "        mean_prior: mean values of the conditional latent prior distributions\n",
    "        log_variance_prior: ln of the variances of the conditional latent prior distributions.\n",
    "        eps: Value added to the denominator for numerical stability. Default: 1e-5.\n",
    "\n",
    "    Returns:\n",
    "        MSE. Reconstruction loss.\n",
    "        Kullback-Leibler Divergence (KLD). KLD between prior distribution and latent\n",
    "            distribution.\n",
    "        Kullback-Leibler Divergence (KLD). KLD between prior distribution and standard normal\n",
    "            distribution\n",
    "\n",
    "    \"\"\"\n",
    "    # log probability for reconstruction\n",
    "    p_x_z = torch.distributions.Normal(loc=x, scale=torch.ones_like(x) * 0.6)\n",
    "    log_prob_p_x_z = p_x_z.log_prob(reconstruction)\n",
    "\n",
    "    mse_reconstruction = -torch.sum(torch.mean(log_prob_p_x_z, dim=(1, 2, 3)))\n",
    "\n",
    "    kld_prior_normal = -0.5 * torch.sum(torch.mean(\n",
    "        1 + log_variance_prior - mean_prior.pow(2) - log_variance_prior.exp()\n",
    "        , dim=1))\n",
    "    kld_prior_latent = torch.sum(torch.mean(\n",
    "        0.5 * (log_variance_prior - log_variance\n",
    "               + (log_variance.exp() + (mean - mean_prior).pow(2)) /\n",
    "               (log_variance_prior.exp() + eps) - 1.)\n",
    "        , dim=1))\n",
    "\n",
    "    return mse_reconstruction, kld_prior_latent, kld_prior_normal\n",
    "\n",
    "def loss_function_consistency(mean_decoder: torch.tensor,\n",
    "                              log_variance_decoder: torch.tensor,\n",
    "                              mean: torch.tensor,\n",
    "                              log_variance: torch.tensor,\n",
    "                              eps: float = 1.0e-5) \\\n",
    "        -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Calculates the consistency loss for counterfactual examples and uses\n",
    "    Kullback-Leibler Divergence between reconstruction x^delta and \n",
    "    input q(z|x) that is modified by the CF method. \n",
    "    \n",
    "    Args:\n",
    "        mean_decoder: Mean values of the latent space for each sample after resconstruction and gda 'x^delta'. (reconstruction)\n",
    "        log_variance_decoder: ln of the variances of the latent space for each sample in 'x^delta' (reconstruction)\n",
    "        mean: Mean values of the latent space for each sample in 'x'.\n",
    "        log_variance: ln of the variances of the latent space for each sample in 'x'\n",
    "        eps: Value added to the denominator for numerical stability. Default: 1e-5.\n",
    "\n",
    "    Returns:\n",
    "        Kullback-Leibler Divergence (KLD). KLD between q(z|x^delta) and q(z^delta|x) distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate KLD for two multivariate normal distributions (latent distribution from input and decoder)\n",
    "    # 0.5 * torch.sum(log(std^2)+(mean^2)-1-std^2)\n",
    "    kld_latent_consistency = torch.sum(torch.mean(\n",
    "        0.5 * (log_variance - log_variance_decoder\n",
    "               + (log_variance_decoder.exp() + (mean_decoder - mean).pow(2)) /\n",
    "               (log_variance.exp() + eps) - 1.)\n",
    "        , dim=1))\n",
    "\n",
    "    return kld_latent_consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Routine and Test Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:53:27.107303Z",
     "start_time": "2024-06-24T06:53:27.086044Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_single_epoch(model: GDVAE, opti: torch.optim.Adam, with_generalized: bool) -> Tuple[torch.tensor, int, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Wrapper for single train epoch\n",
    "    Args:\n",
    "        model: GDVAE model to train\n",
    "        opti: Optimizer of GDVAE model\n",
    "        with_generalized: If the generalized cf generation should be included during training\n",
    "    Returns:\n",
    "        torch.tensor: loss of epoch\n",
    "        int: Accuracy of GDA\n",
    "        torch.tensor: reconstruction loss of vae\n",
    "\n",
    "    \"\"\"\n",
    "    loss, acc, mse = single_epoch(model, opti, train=True, include_generalized=with_generalized)\n",
    "    return loss, acc, mse\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_single_epoch(model: GDVAE, opti: torch.optim.Adam, with_generalized: bool) -> Tuple[torch.tensor, int, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Wrapper to disable Gradiant Compution on test run\n",
    "    Args:\n",
    "        model: GDVAE model to train\n",
    "        opti: Optimizer of GDVAE model\n",
    "        with_generalized: If the generalized cf generation should be included during training\n",
    "    Returns:\n",
    "        torch.tensor: loss of epoch\n",
    "        int: Accuracy of GDA\n",
    "        torch.tensor: reconstruction loss of vae\n",
    "\n",
    "    \"\"\"\n",
    "    loss, acc, mse = single_epoch(model, opti, train=False, include_generalized=with_generalized)\n",
    "    return loss, acc, mse\n",
    "\n",
    "def single_epoch(model: GDVAE, opti: torch.optim.Adam, train: bool=True, include_generalized: bool=False) -> Tuple[torch.tensor, int, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Train a model for a single epoch.\n",
    "\n",
    "    Args:\n",
    "        model (GDVAE): Model\n",
    "        opti (torch.optim.Adam): Optimizer\n",
    "        train: running single epoch in training or test mode\n",
    "        include_generalized: If the generalized cf generation should be included during training\n",
    "    Returns:\n",
    "        (torch.tensor): training loss\n",
    "        (int): training accuracy\n",
    "        (torch.tensor): reconstruction loss of vae\n",
    "    \"\"\"\n",
    "    y_preds_cat = torch.tensor([]).to(device)  # GDA class predictions\n",
    "    y_gts = torch.tensor([]).to(device)  # ground truth class labels\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_loss_cvae_mse = 0\n",
    "    running_counter = 0\n",
    "    if train:\n",
    "        loader = model.train_loader\n",
    "    else:\n",
    "        loader = model.test_loader\n",
    "\n",
    "    for (batch_data, y) in tqdm(loader):\n",
    "        batch_data = batch_data.to(device)  # (n,1,h,w)\n",
    "        y = y.to(device).squeeze()\n",
    "        y_one_hot = torch.nn.functional.one_hot(y, num_classes=model.num_classes).float().to(device).squeeze()  # (n, k)\n",
    "        if train:\n",
    "            opti.zero_grad()\n",
    "\n",
    "        y_preds, z_preds, y_preds_hard, enc_means, enc_logvars = model.gda(batch_data)\n",
    "\n",
    "        y_gts = torch.cat([y_gts, y], dim=0)\n",
    "        y_preds_cat = torch.cat([y_preds_cat, y_preds.detach()], dim=0)\n",
    "\n",
    "        mean_prior, log_variance_prior = model.prior_network(y_one_hot)  # --> p(y)\n",
    "        mean, log_variance = model.encode(batch_data, y_one_hot)  # --> q(z|x,y)\n",
    "\n",
    "        reconstruction = model.decode(z_preds, y_preds_hard)  # (n,1,h,w) --> p(x|z,y)\n",
    "\n",
    "        # classification_loss\n",
    "        classification_loss = torch.nn.functional.cross_entropy(y_preds, y, reduction='sum')\n",
    "\n",
    "        # reconstruction loss, kld, kld_prior\n",
    "        cvae_mse, cvae_kld, cvae_kld_prior = loss_function_cvae(batch_data, reconstruction, mean, log_variance,\n",
    "                                                                mean_prior, log_variance_prior)\n",
    "\n",
    "        loss_cvae = (model.ALPHA + model.BETA) * cvae_mse +\\\n",
    "                    model.ALPHA * model.kld_weight * cvae_kld +\\\n",
    "                    model.BETA * model.kld_weight * cvae_kld_prior\n",
    "\n",
    "        loss_ce = model.ce_weight * model.BETA * classification_loss\n",
    "\n",
    "        # consistency loss\n",
    "        _, _, _, enc_means_decoder, enc_logvars_decoder = model.gda(reconstruction)\n",
    "\n",
    "        consistency_kld = loss_function_consistency(enc_means_decoder, enc_logvars_decoder, enc_means,\n",
    "                                                    enc_logvars) / (model.sample_amount_cf + 1)\n",
    "\n",
    "        # calculate consistency loss to q(z|x) (cycle consistency for reconstructions and counterfactuals)\n",
    "        consistency_delta_kld = torch.tensor(0.).to(device)\n",
    "        # Pregenerate w and b\n",
    "        model.cf_generator.mean_prior, model.cf_generator.std_prior = model.prior_network.get_prior_distributions_parameters()\n",
    "        model.cf_generator.calculate_w_and_b()\n",
    "        delta_distribution = model.cf_generator.calculate_counterfactual_delta_distribution(enc_means, y_one_hot)   \n",
    "        # Create 10 Counterfactuals for every Image\n",
    "        for i in range(0, model.sample_amount_cf):\n",
    "            delta = delta_distribution.rsample()\n",
    "            delta = delta.view(batch_size, 1)\n",
    "            y_preds_delta = get_prediction_delta(y_preds, delta)\n",
    "            if include_generalized:\n",
    "                random_mode = torch.randint(0, 3, ())\n",
    "            else:\n",
    "                random_mode = torch.randint(0, 2, ())\n",
    "            # Randomly switch between local_l2 and glob\n",
    "            if random_mode == 0:\n",
    "                enc_means_delta = model.cf_generator.local_l2( enc_means, y_one_hot, delta)\n",
    "            elif random_mode == 1:\n",
    "                enc_means_delta = model.cf_generator.glob( enc_means, y_one_hot, delta)\n",
    "            elif random_mode == 2:\n",
    "                enc_means_delta = model.cf_generator.local_m( enc_means, y_one_hot, delta)\n",
    "            else:\n",
    "                print(\"Random_Mode Error\")\n",
    "                raise ValueError\n",
    "            \n",
    "            enc_logvars_delta = enc_logvars.clone()\n",
    "\n",
    "            x_deltas_rec = model.decode(enc_means_delta, y_preds_delta)  # (n,1,h,w)\n",
    "\n",
    "            _, _, _, enc_means_delta_rec, enc_logvars_delta_rec = model.gda(x_deltas_rec)\n",
    "\n",
    "            consistency_delta_kld += loss_function_consistency(enc_means_delta_rec,\n",
    "                                                               enc_logvars_delta_rec,\n",
    "                                                               enc_means_delta,\n",
    "                                                               enc_logvars_delta) / (model.sample_amount_cf + 1)\n",
    "\n",
    "        loss_consistency_kld = model.kld_weight * model.GAMMA * consistency_kld + \\\n",
    "                               model.kld_weight * model.GAMMA * consistency_delta_kld\n",
    "\n",
    "        loss = loss_cvae + loss_ce + loss_consistency_kld\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "\n",
    "        running_counter += batch_data.shape[0]\n",
    "        running_loss_cvae_mse += (model.ALPHA + model.BETA) * cvae_mse.item()\n",
    "        running_loss += loss.item()\n",
    "    y_preds_cat = torch.argmax(y_preds_cat, dim=1).cpu()\n",
    "    accuracy = accuracy_score(y_gts.cpu(), y_preds_cat)\n",
    "\n",
    "    return (running_loss/running_counter), accuracy, (running_loss_cvae_mse/running_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GdVAE (CelebA)\n",
    "To test the counterfactual generation process without prior training, we provide pre-trained weights (gdvae_celeba_pretrained.pth). Thus, it is unnecessary to perform this step, and you may proceed directly to the final stage of generating counterfactuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-21T10:21:41.709966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454c46391dbf42deb7683f8e3bd8682a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdvae = GDVAE().to(device)\n",
    "\n",
    "EPOCHS = 24\n",
    "\n",
    "# Include the Generalized Counterfactual Generation during training?\n",
    "# NOTE: Paper results don't include the generalized cf (local_m) generation during training\n",
    "with_generalized = False\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(gdvae.parameters(),\n",
    "                             lr=0.0005,\n",
    "                             weight_decay=0.0)\n",
    "for epoch in range(EPOCHS):\n",
    "    gdvae.train()\n",
    "    loss, acc, loss_mse = train_single_epoch(gdvae, optimizer, with_generalized)\n",
    "    print(f\"Epoch {epoch:04d} \"\n",
    "          f\"train_loss {(loss):.4f} \"\n",
    "          f\"train_accuracy {acc:.4f} \"\n",
    "          f\"train_mse {loss_mse:.4f}\")\n",
    "    gdvae.eval()\n",
    "    loss, acc, loss_mse = test_single_epoch(gdvae, optimizer, with_generalized)\n",
    "    print(f\"Epoch {epoch:04d} \"\n",
    "          f\"test_loss {(loss):.4f} \"\n",
    "          f\"test_accuracy {acc:.4f} \"\n",
    "          f\"test_mse {loss_mse:.4f}\")\n",
    "    torch.save(gdvae.state_dict(), f\"./gdvae_celeba.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Counterfactuals\n",
    "The counterfactual generation mode can be switched between global and local_l2 by changing cf_mode to True(global) or False(local_l2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:54:45.998449Z",
     "start_time": "2024-06-24T06:54:44.836787Z"
    }
   },
   "outputs": [],
   "source": [
    "gdvae = GDVAE().to(device)\n",
    "\n",
    "# Load Weights\n",
    "gdvae.load_state_dict(torch.load(f\"./gdvae_celeba_pretrained.pth\", map_location=device))\n",
    "gdvae.eval()\n",
    "\n",
    "# Counterfactual Mode\n",
    "# Global = True\n",
    "# Local_l2 = False\n",
    "cf_mode = False\n",
    "\n",
    "# Counterfactual probabilities\n",
    "cf_probs = np.array([0.95, 0.75, 0.5, 0.25, 0.05])\n",
    "\n",
    "# Figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "with torch.no_grad():\n",
    "    (batch_data, y) = next(iter(gdvae.test_loader))\n",
    "    batch_data = batch_data.to(device)\n",
    "    plt.imshow(torch.permute(normalize(batch_data[0]), (1, 2, 0)).cpu().detach().numpy())\n",
    "    axs[0].imshow(torch.permute(normalize(batch_data[0]), (1, 2, 0)).cpu().detach().numpy())\n",
    "    axs[0].set_title(f\"Input\")\n",
    "    axs[0].set_xticklabels([]),axs[0].set_yticklabels([])\n",
    "    y = y.to(device).squeeze()\n",
    "    y_one_hot = torch.nn.functional.one_hot(y, num_classes=gdvae.num_classes).float().to(device).squeeze()\n",
    "    _, _, _, enc_means, _ = gdvae.gda(batch_data)\n",
    "    reco = gdvae.decode(enc_means, y_one_hot)\n",
    "    axs[1].imshow(torch.permute(normalize(reco[0]), (1, 2, 0)).cpu().detach().numpy())\n",
    "    axs[1].set_title(f\"Recon\")\n",
    "    axs[1].set_xticklabels([]),axs[0].set_yticklabels([])\n",
    "    plt.show()\n",
    "    fig, axs = plt.subplots(1, len(cf_probs), figsize=(15, 8))\n",
    "    gdvae.cf_generator.mean_prior, gdvae.cf_generator.std_prior = gdvae.prior_network.get_prior_distributions_parameters()\n",
    "    gdvae.cf_generator.calculate_w_and_b()\n",
    "    for i, probs in enumerate(cf_probs):\n",
    "        probvalue = torch.log(torch.tensor([probs]) / (1 - torch.tensor([probs]))).float().to(device)\n",
    "        delta = probvalue.repeat(enc_means.shape[0], 1)\n",
    "        y_pred_delta_gt = get_prediction_delta(y_one_hot, delta)\n",
    "        if cf_mode:\n",
    "            z_deltas = gdvae.cf_generator.glob(enc_means, y_one_hot, delta)\n",
    "        else:\n",
    "            z_deltas = gdvae.cf_generator.local_l2(enc_means, y_one_hot, delta)\n",
    "        counterfactual = gdvae.decode(z_deltas.to(torch.float32), y_pred_delta_gt)\n",
    "        axs[i].imshow(torch.permute(normalize(counterfactual[0]), (1, 2, 0)).cpu().detach().numpy())\n",
    "        axs[i].set_title(f\"Prob:{probs}\")\n",
    "        axs[i].set_xticklabels([]),axs[i].set_yticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generating Counterfactuals (Generalized,local_m)\n",
    "Create Counterfactuals with the generalized cf generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T06:54:32.800598Z",
     "start_time": "2024-06-24T06:54:30.809114Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdvae = GDVAE().to(device)\n",
    "\n",
    "# Load Weights\n",
    "gdvae.load_state_dict(torch.load(f\"./gdvae_celeba_pretrained.pth\", map_location=device))\n",
    "gdvae.eval()\n",
    "# Counterfactual Mode\n",
    "# Global = True\n",
    "# Local = False\n",
    "cf_mode = True\n",
    "\n",
    "# Counterfactual probabilities\n",
    "cf_probs = np.array([0.95, 0.75, 0.5, 0.25, 0.05])\n",
    "\n",
    "# Figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "with torch.no_grad():\n",
    "    (batch_data, y) = next(iter(gdvae.test_loader))\n",
    "    batch_data = batch_data.to(device)\n",
    "    plt.imshow(torch.permute(normalize(batch_data[0]), (1, 2, 0)).cpu().detach().numpy())\n",
    "    axs[0].imshow(torch.permute(normalize(batch_data[0]), (1, 2, 0)).cpu().detach().numpy())\n",
    "    axs[0].set_title(f\"Input\")\n",
    "    axs[0].set_xticklabels([]),axs[0].set_yticklabels([])\n",
    "    y = y.to(device).squeeze()\n",
    "    y_one_hot = torch.nn.functional.one_hot(y, num_classes=gdvae.num_classes).float().to(device).squeeze()\n",
    "    _, _, _, enc_means, _ = gdvae.gda(batch_data)\n",
    "    reco = gdvae.decode(enc_means, y_one_hot)\n",
    "    axs[1].imshow(torch.permute(normalize(reco[0]), (1, 2, 0)).cpu().detach().numpy())\n",
    "    axs[1].set_title(f\"Recon\")\n",
    "    axs[1].set_xticklabels([]),axs[0].set_yticklabels([])\n",
    "    plt.show()\n",
    "    fig, axs = plt.subplots(1, len(cf_probs), figsize=(15, 8))\n",
    "    gdvae.cf_generator.mean_prior, gdvae.cf_generator.std_prior = gdvae.prior_network.get_prior_distributions_parameters()\n",
    "    gdvae.cf_generator.calculate_w_and_b()\n",
    "    for i, probs in enumerate(cf_probs):\n",
    "        probvalue = torch.log(torch.tensor([probs]) / (1 - torch.tensor([probs]))).float().to(device)\n",
    "        delta = probvalue.repeat(enc_means.shape[0], 1)\n",
    "        y_pred_delta_gt = get_prediction_delta(y_one_hot, delta)\n",
    "        z_deltas  = gdvae.cf_generator.local_m(enc_means, y_one_hot, delta)\n",
    "        counterfactual = gdvae.decode(z_deltas.to(torch.float32), y_pred_delta_gt)\n",
    "        axs[i].imshow(torch.permute(normalize(counterfactual[0]), (1, 2, 0)).cpu().detach().numpy())\n",
    "        axs[i].set_title(f\"Prob:{probs}\")\n",
    "        axs[i].set_xticklabels([]),axs[i].set_yticklabels([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
